# ğŸ§ª Guia de Experimentos AvanÃ§ados - MLP MNIST (64 neurÃ´nios)

Este guia descreve os novos experimentos disponÃ­veis para anÃ¡lise aprofundada da rede neural MLP com 64 neurÃ´nios na camada oculta.

## ğŸ“Š Experimentos DisponÃ­veis

### 1. **VariaÃ§Ã£o de NÃ­veis de RuÃ­do** ğŸ”Š
Testa a robustez da rede contra diferentes intensidades de ruÃ­do gaussiano.

- **ParÃ¢metros testados**: Ïƒ = 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4
- **Objetivo**: Identificar o ponto de ruptura de performance
- **AnÃ¡lise**: Curva de degradaÃ§Ã£o vs. intensidade do ruÃ­do

### 2. **VariaÃ§Ã£o de Taxas de Aprendizado** ğŸ“ˆ
Investiga o impacto da taxa de aprendizado na convergÃªncia e robustez.

- **ParÃ¢metros testados**: 1e-4, 5e-4, 1e-3, 5e-3, 1e-2
- **Objetivo**: Encontrar o equilÃ­brio entre velocidade de convergÃªncia e estabilidade
- **AnÃ¡lise**: Tempo de treinamento, accuracy final e overfitting

### 3. **VariaÃ§Ã£o de Tamanhos de Batch** ğŸ“¦
Analisa o efeito do batch size no treinamento e generalizaÃ§Ã£o.

- **ParÃ¢metros testados**: 32, 64, 128, 256, 512
- **Objetivo**: Avaliar trade-off entre tempo de treinamento e qualidade do modelo
- **AnÃ¡lise**: ConvergÃªncia, tempo de treinamento e estabilidade

### 4. **VariaÃ§Ã£o de Taxas de Dropout** ğŸ¯
Testa o efeito da regularizaÃ§Ã£o via dropout na robustez ao ruÃ­do.

- **ParÃ¢metros testados**: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5
- **Objetivo**: Determinar se dropout melhora a robustez ao ruÃ­do gaussiano
- **AnÃ¡lise**: Trade-off entre overfitting e underfitting

### 5. **VariaÃ§Ã£o de FunÃ§Ãµes de AtivaÃ§Ã£o** âš¡
Compara diferentes funÃ§Ãµes de ativaÃ§Ã£o na camada oculta.

- **FunÃ§Ãµes testadas**: ReLU, Tanh, ELU, Sigmoid
- **Objetivo**: Identificar qual ativaÃ§Ã£o oferece melhor robustez
- **AnÃ¡lise**: Gradient flow, convergÃªncia e robustez

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: Executar TODOS os Experimentos

```bash
python advanced_experiments.py
```

Isso executarÃ¡ todos os 5 experimentos sequencialmente e gerarÃ¡:
- Resultados em JSON
- RelatÃ³rios em TXT
- GrÃ¡ficos comparativos em PNG
- Resumo geral

**Tempo estimado**: ~30-45 minutos (depende do hardware)

### OpÃ§Ã£o 2: Executar Experimentos EspecÃ­ficos

```bash
# Executar apenas experimentos de ruÃ­do e dropout
python run_custom_experiments.py --experiments noise dropout

# Executar apenas experimento de learning rate
python run_custom_experiments.py --experiments learning_rate

# Executar todos
python run_custom_experiments.py --all
```

### OpÃ§Ã£o 3: Experimento Customizado

```bash
# Exemplo: Testar configuraÃ§Ã£o especÃ­fica
python run_custom_experiments.py --custom \
  --initializer he \
  --optimizer adam \
  --learning-rate 0.001 \
  --dropout 0.3 \
  --noise 0.25 \
  --activation relu \
  --batch-size 128 \
  --epochs 20 \
  --name "meu_experimento"
```

## ğŸ“‚ Estrutura de SaÃ­das

Cada execuÃ§Ã£o cria uma pasta com timestamp:

```
outputs_advanced/
â””â”€â”€ experiment_YYYYMMDD_HHMMSS/
    â”œâ”€â”€ models/              # Modelos salvos (.h5)
    â”œâ”€â”€ plots/               # GrÃ¡ficos comparativos
    â”‚   â”œâ”€â”€ noise_levels_comparison.png
    â”‚   â”œâ”€â”€ learning_rates_comparison.png
    â”‚   â”œâ”€â”€ batch_sizes_comparison.png
    â”‚   â”œâ”€â”€ dropout_rates_comparison.png
    â”‚   â””â”€â”€ activations_comparison.png
    â”œâ”€â”€ results/             # Resultados em JSON e TXT
    â”‚   â”œâ”€â”€ noise_levels_results.json
    â”‚   â”œâ”€â”€ noise_levels_summary.txt
    â”‚   â”œâ”€â”€ learning_rates_results.json
    â”‚   â”œâ”€â”€ learning_rates_summary.txt
    â”‚   â”œâ”€â”€ batch_sizes_results.json
    â”‚   â”œâ”€â”€ batch_sizes_summary.txt
    â”‚   â”œâ”€â”€ dropout_rates_results.json
    â”‚   â”œâ”€â”€ dropout_rates_summary.txt
    â”‚   â”œâ”€â”€ activations_results.json
    â”‚   â”œâ”€â”€ activations_summary.txt
    â”‚   â””â”€â”€ overall_summary.txt
    â””â”€â”€ logs/                # Logs de execuÃ§Ã£o
```

## ğŸ“ˆ MÃ©tricas Analisadas

Para cada experimento, as seguintes mÃ©tricas sÃ£o coletadas:

1. **Accuracy em dados limpos** (baseline)
2. **Accuracy em dados ruidosos** (robustez)
3. **DegradaÃ§Ã£o percentual** (clean_acc - noisy_acc)
4. **Tempo de treinamento** (segundos)
5. **NÃºmero de Ã©pocas** (atÃ© early stopping)
6. **Loss de validaÃ§Ã£o final**
7. **HistÃ³rico completo** (loss e accuracy por Ã©poca)

## ğŸ¨ VisualizaÃ§Ãµes Geradas

Cada experimento gera um grÃ¡fico com 4 painÃ©is:

1. **Top-left**: Accuracy vs. ParÃ¢metro (limpo e ruidoso)
2. **Top-right**: DegradaÃ§Ã£o vs. ParÃ¢metro
3. **Bottom-left**: Tempo de Treinamento vs. ParÃ¢metro
4. **Bottom-right**: Tabela com mÃ©tricas detalhadas

## ğŸ’¡ Dicas de Uso

### Para AnÃ¡lise RÃ¡pida
```bash
# Execute apenas o experimento de interesse
python run_custom_experiments.py --experiments noise
```

### Para AnÃ¡lise Completa
```bash
# Execute todos os experimentos
python advanced_experiments.py
```

### Para Teste de HipÃ³tese EspecÃ­fica
```bash
# Teste uma configuraÃ§Ã£o especÃ­fica
python run_custom_experiments.py --custom \
  --dropout 0.4 \
  --noise 0.3 \
  --name "teste_dropout_alto"
```

## ğŸ“Š Exemplos de AnÃ¡lises

### Exemplo 1: Encontrar o Melhor Learning Rate
```bash
python run_custom_experiments.py --experiments learning_rate
# Analise os grÃ¡ficos em outputs_advanced/experiment_*/plots/learning_rates_comparison.png
```

### Exemplo 2: Testar RegularizaÃ§Ã£o
```bash
python run_custom_experiments.py --experiments dropout
# Compare degradaÃ§Ã£o com e sem dropout
```

### Exemplo 3: Robustez Extrema
```bash
# Teste com ruÃ­do muito alto
python run_custom_experiments.py --custom \
  --noise 0.5 \
  --dropout 0.3 \
  --name "robustez_extrema"
```

## ğŸ” InterpretaÃ§Ã£o dos Resultados

### DegradaÃ§Ã£o Baixa (< 5%)
- Modelo muito robusto ao ruÃ­do
- Boa generalizaÃ§Ã£o
- Provavelmente nÃ£o estÃ¡ overfitting

### DegradaÃ§Ã£o MÃ©dia (5-15%)
- Robustez aceitÃ¡vel
- Comportamento esperado para MNIST
- Trade-off razoÃ¡vel

### DegradaÃ§Ã£o Alta (> 15%)
- Modelo sensÃ­vel ao ruÃ­do
- Pode estar overfitting
- Considere regularizaÃ§Ã£o adicional

## ğŸ§  ConfiguraÃ§Ãµes Recomendadas

Com base nos experimentos, configuraÃ§Ãµes tÃ­picas bem-sucedidas:

### Para MÃ¡xima Accuracy
```python
initializer='he'
optimizer='adam'
learning_rate=1e-3
batch_size=128
dropout=0.0
activation='relu'
```

### Para MÃ¡xima Robustez
```python
initializer='he'
optimizer='adam'
learning_rate=5e-4
batch_size=64
dropout=0.2
activation='relu'
```

### Para Treinamento RÃ¡pido
```python
initializer='he'
optimizer='adam'
learning_rate=1e-3
batch_size=256
dropout=0.1
activation='relu'
```

## ğŸ“š ReferÃªncias e Insights

### Sobre InicializaÃ§Ã£o
- **He**: Melhor para ReLU e ELU
- **Glorot**: Melhor para Tanh e Sigmoid

### Sobre Dropout
- Dropout alto (>0.3) pode prejudicar em redes pequenas
- Dropout moderado (0.2-0.3) geralmente melhora robustez

### Sobre Learning Rate
- LR muito alto: convergÃªncia instÃ¡vel
- LR muito baixo: treinamento lento
- Sweet spot geralmente entre 5e-4 e 1e-3

### Sobre Batch Size
- Batch pequeno: mais ruÃ­do no gradiente, pode generalizar melhor
- Batch grande: convergÃªncia mais suave, mais rÃ¡pido por Ã©poca

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

Para modificar experimentos, edite diretamente o arquivo `advanced_experiments.py`:

```python
# Exemplo: Adicionar mais nÃ­veis de ruÃ­do
noise_levels = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]

# Exemplo: Testar L2 regularization
config['regularizer'] = 'l2'
config['reg_strength'] = 0.01
```

## ğŸ› Troubleshooting

### Erro de MemÃ³ria
- Reduza o batch size
- Reduza o nÃºmero de experimentos simultÃ¢neos

### Treinamento Muito Lento
- Aumente o batch size
- Reduza validation_split
- Use GPU se disponÃ­vel

### Early Stopping Muito Cedo
- Aumente patience
- Reduza min_delta
- Ajuste learning rate

## ğŸ“ Suporte

Para dÃºvidas ou sugestÃµes:
- Abra uma issue no GitHub
- Consulte a documentaÃ§Ã£o do TensorFlow
- Revise os logs em `advanced_experiment.log`

---

**Desenvolvido para anÃ¡lise sistemÃ¡tica de redes neurais MLP** ğŸš€

